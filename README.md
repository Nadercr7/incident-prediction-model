<div align="center">

# ğŸ“Š Incident Count Prediction Model

<p>
  <img src="https://img.shields.io/badge/Python-3.9+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/scikit--learn-1.3+-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white" alt="scikit-learn">
  <img src="https://img.shields.io/badge/Model-Gradient_Boosting-success?style=for-the-badge" alt="Model">
  <img src="https://img.shields.io/badge/RÂ²-0.9282-blue?style=for-the-badge" alt="RÂ²">
  <img src="https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge" alt="License">
</p>

<p><strong>Time-series regression model predicting operational incident counts across 116 locations<br>using a heavily regularized Gradient Boosting pipeline â€” built on 20,000+ real-world records.</strong></p>

<p><em>Developed during a Data Science Internship at <strong>Zetta Global</strong> Â· February 2026</em></p>

</div>

---

## ğŸ¯ Results at a Glance

<table>
  <tr>
    <td align="center"><h3>0.9282</h3><sub>RÂ² Score</sub></td>
    <td align="center"><h3>20.99</h3><sub>MAE</sub></td>
    <td align="center"><h3>14.00</h3><sub>Median AE</sub></td>
    <td align="center"><h3>30.56</h3><sub>RMSE</sub></td>
    <td align="center"><h3>67%</h3><sub>vs Baseline</sub></td>
  </tr>
</table>

> **93% of variance explained** â€” the model reduces prediction error by **67%** compared to a naive mean predictor (MAE: 20.99 vs baseline 64.54).

---

## ğŸ§© Problem Statement

A transportation company needed to **forecast incident volumes per location for the upcoming year** to enable proactive resource allocation, staffing decisions, and risk mitigation â€” shifting from **reactive response** to **data-driven prevention**.

<table>
  <tr>
    <td>âš ï¸ <strong>Skewed Distribution</strong><br><sub>Top location had 10Ã— the median count</sub></td>
    <td>ğŸ“‰ <strong>Limited History</strong><br><sub>Only 3 years of usable data (232 training samples)</sub></td>
    <td>ğŸ” <strong>Leakage Risk</strong><br><sub>Same-year aggregates had 99%+ target correlation</sub></td>
  </tr>
</table>

---

## ğŸ”¬ Approach

### Pipeline

```
Raw Data â”€â”€â–¶ Clean & Preprocess â”€â”€â–¶ Feature Engineering â”€â”€â–¶ Feature Selection â”€â”€â–¶ Model Training â”€â”€â–¶ Validation â”€â”€â–¶ Predictions
  20K+          Temporal features       56 features            Top 20 (RF+ET)       Gradient Boosting   Temporal split   8,744 forecasts
 records        Binary flags            4 categories           averaged importance   log-transformed     strict pastâ†’future
```

### 1ï¸âƒ£ Data Processing

| Aspect | Detail |
|:-------|:-------|
| **Records** | 20,000+ real-world incident records |
| **Timespan** | 3 years (2023â€“2025) |
| **Locations** | 116 unique sites |
| **Processing** | Temporal feature extraction, binary flags, missing value handling |

### 2ï¸âƒ£ Feature Engineering â€” 56 features â†’ 20 selected

<table>
  <tr>
    <th width="200">Category</th>
    <th>Description</th>
    <th width="180">Impact</th>
  </tr>
  <tr>
    <td>ğŸ”— <strong>Interaction Features</strong></td>
    <td>Trend Ã— magnitude, weighted trends, ratio features</td>
    <td><img src="https://img.shields.io/badge/â˜…â˜…â˜…-Highest_Impact-success?style=flat-square" alt="highest"></td>
  </tr>
  <tr>
    <td>âª <strong>Lag Features</strong></td>
    <td>Prior-year counts (raw, log, sqrt), damage/collision rates</td>
    <td><img src="https://img.shields.io/badge/â˜…â˜…â˜…-Core_Predictors-blue?style=flat-square" alt="core"></td>
  </tr>
  <tr>
    <td>ğŸŒ¡ï¸ <strong>Seasonal Patterns</strong></td>
    <td>Quarterly distributions, peak quarter, half-year proportions</td>
    <td><img src="https://img.shields.io/badge/â˜…â˜…-Temporal_Signal-orange?style=flat-square" alt="temporal"></td>
  </tr>
  <tr>
    <td>ğŸ“ˆ <strong>Historical Stats</strong></td>
    <td>Rolling averages, min/max/mean, trend acceleration, CV</td>
    <td><img src="https://img.shields.io/badge/â˜…â˜…-Stability_Signal-orange?style=flat-square" alt="stability"></td>
  </tr>
</table>

> Feature selection via **averaged Random Forest + ExtraTrees importance scores** â€” two-model averaging reduces selection bias.

### 3ï¸âƒ£ Model: Pure Gradient Boosting

After testing Random Forest, Ridge, SVR, ensembles, and stacking â€” a **single, heavily regularized Gradient Boosting** model outperformed all combinations:

```python
GradientBoostingRegressor(
    n_estimators=150,      # conservative count
    max_depth=3,           # shallow trees prevent overfitting
    learning_rate=0.03,    # slow learning for better generalization
    subsample=0.8,         # stochastic gradient boosting
    min_samples_leaf=4,    # regularization
    random_state=42
)
```

> **Why this works:** With only **232 training samples**, a simple well-tuned model generalizes better than complex ensembles. The key was **log-transforming the target** to handle right-skewed count distributions.

### 4ï¸âƒ£ Validation Strategy

| Strategy | Purpose |
|:---------|:--------|
| ğŸ• **Strict temporal split** | Model only ever sees the past â€” no random CV |
| ğŸ“ **Log-transformed target** | Handles right-skewed distribution of counts |
| ğŸ›¡ï¸ **Data leakage detection** | Caught & removed same-year features with 99%+ target correlation |

---

## ğŸ“Š Top Feature Importances

```
  Weighted Trend        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  18.4%
  Trend Magnitude       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  15.0%
  Peak Quarter Count    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  13.4%
  Key Incident (lag)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  12.8%
  Year-over-Year Trend  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   8.2%
  Seasonal Variance     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   6.3%
  High-Volume Flag      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   5.1%
```

> **Interaction features** (weighted trend, trend Ã— magnitude) dominate â€” this was the key breakthrough that pushed RÂ² from ~0.80 to 0.93.

---

## ğŸ”® Prediction Summary (2026)

<div align="center">

| | Metric | Value |
|:---:|:-------|------:|
| ğŸ“‹ | **Total Predicted Incidents** | **8,744** |
| ğŸ¢ | **Locations Covered** | **116** |
| ğŸ“ | **Average per Location** | **~75** |

</div>

### Distribution Across Locations

```
  200+ incidents  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   6 sites   (high-risk)
  100â€“199         â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  17 sites
  50â€“99           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  38 sites   (largest group)
  20â€“49           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  29 sites
  < 20            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  26 sites   (low-volume)
```

---

## ğŸ“ Project Structure

```
incident-prediction-model/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # You are here
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT License
â”œâ”€â”€ ğŸ“„ requirements.txt             # Dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                   # Privacy & cleanup rules
â”‚
â”œâ”€â”€ âš™ï¸ config/
â”‚   â””â”€â”€ config.py                   # Hyperparameters, feature lists, paths
â”‚
â”œâ”€â”€ ğŸ“¦ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py              # CSV loading & preprocessing
â”‚   â”œâ”€â”€ feature_engineering.py      # 56 features across 4 categories
â”‚   â”œâ”€â”€ model_training.py           # GB training with log-transformed target
â”‚   â”œâ”€â”€ model_evaluation.py         # RÂ², MAE, RMSE, sMAPE, baseline comparison
â”‚   â””â”€â”€ prediction.py               # Future-year prediction generation
â”‚
â”œâ”€â”€ ğŸš€ main.py                     # Full pipeline entry point (CLI)
â”‚
â”œâ”€â”€ ğŸ“Š data/
â”‚   â””â”€â”€ sample_data.csv             # Synthetic sample (real data excluded)
â”‚
â”œâ”€â”€ ğŸ“ˆ output/                     # Generated predictions (gitignored)
â””â”€â”€ ğŸ¤– models/                     # Saved models (gitignored)
```

---

## ğŸš€ Getting Started

### Installation

```bash
git clone https://github.com/Nadercr7/incident-prediction-model.git
cd incident-prediction-model
python -m venv .venv
.venv\Scripts\activate          # Windows
# source .venv/bin/activate     # macOS/Linux
pip install -r requirements.txt
```

### Run the Full Pipeline

```bash
python main.py                                  # default settings
python main.py --data-path data/my_data.csv     # custom data
python main.py --year 2027                      # different prediction year
python main.py --config                         # view configuration
python main.py --quiet                          # reduced output
```

<details>
<summary><strong>ğŸ“ Use Individual Modules</strong> (click to expand)</summary>

<br>

```python
from src.data_loader import IncidentDataLoader
from src.feature_engineering import FeatureEngineer
from src.model_training import ModelTrainer
from src.prediction import PredictionGenerator

# 1. Load & preprocess
loader = IncidentDataLoader("data/incidents.csv")
df = loader.process_all()

# 2. Engineer features (56 total)
engineer = FeatureEngineer(df)
df_features = engineer.build_all_features()

# 3. Select top 20 features
features = engineer.select_top_features(n=20)

# 4. Train model
trainer = ModelTrainer()
trainer.prepare_data(df_features, features)
model = trainer.train_gradient_boosting()

# 5. Generate predictions
generator = PredictionGenerator(df_features, features, trainer)
predictions = generator.generate_predictions(2026)
generator.save_predictions("output/predictions_2026.csv")
```

</details>

---

## ğŸ’¡ Key Learnings

<table>
  <tr>
    <td width="60" align="center">ğŸ”§</td>
    <td><strong>Feature engineering > model complexity</strong><br>The jump from ~80% to 93% RÂ² came from <em>interaction features</em> (weighted trends, trend Ã— lag), not from more complex algorithms.</td>
  </tr>
  <tr>
    <td align="center">ğŸ•</td>
    <td><strong>Temporal validation is non-negotiable</strong><br>Random splits in time-series data inflate metrics. Strict pastâ†’future split ensures honest, deployable results.</td>
  </tr>
  <tr>
    <td align="center">ğŸ›¡ï¸</td>
    <td><strong>Data leakage detection saved the project</strong><br>Same-year aggregate features had 99%+ correlation with the target. Without catching this, the model would appear RÂ²=0.99 but fail completely in production.</td>
  </tr>
  <tr>
    <td align="center">ğŸ“‰</td>
    <td><strong>Small data â‰  bad model</strong><br>With only 232 training samples, careful regularization (shallow trees, low learning rate, subsampling) achieved strong generalization.</td>
  </tr>
</table>

---

## âš ï¸ Limitations

| Limitation | Detail |
|:-----------|:-------|
| **Limited history** | 3 years of data constrains lag feature depth |
| **No external factors** | Weather, policy changes, economic conditions not included |
| **Annual granularity** | Monthly/weekly predictions would require more data |
| **Point predictions** | No confidence intervals (could add with quantile regression) |
| **sMAPE = 56.4%** | Inflated by low-count locations where Â±5 incidents = large % error |

---

## ğŸ› ï¸ Tech Stack

<p>
  <img src="https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white" alt="pandas">
  <img src="https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white" alt="NumPy">
  <img src="https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white" alt="scikit-learn">
  <img src="https://img.shields.io/badge/Matplotlib-11557C?style=for-the-badge&logo=matplotlib&logoColor=white" alt="Matplotlib">
  <img src="https://img.shields.io/badge/Seaborn-444876?style=for-the-badge&logo=python&logoColor=white" alt="Seaborn">
</p>

---

## ğŸ”’ Privacy Notice

> This project was completed during a Data Science internship at **Zetta Global**. All location names, employee data, and sensitive operational details have been **removed**. Only methodology, aggregate statistics, and anonymized results are shared. The raw dataset is **not included** in this repository.

---

## ğŸ‘¤ Author

<table>
  <tr>
    <td>
      <strong>Nader Mohamed</strong><br>
      Data Science Intern Â· <strong>Zetta Global</strong> Â· February 2026<br><br>
      <a href="https://github.com/Nadercr7"><img src="https://img.shields.io/badge/GitHub-Nadercr7-181717?style=flat-square&logo=github" alt="GitHub"></a>
    </td>
  </tr>
</table>

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

<div align="center">
  <sub>â­ If you found this project useful, consider giving it a star!</sub>
</div>
